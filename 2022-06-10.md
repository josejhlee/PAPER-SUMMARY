# 2022-06-10

## Artistic Style Transfer with Internal-external Learning and Contrastive Learning

Further develops SANET, a GAN architecture for style transfer which takes input content and style image, and outputs a mixed image. 
Here, the paper argues that style(content) image should have closer relations to another image with the same style(content), and they call this "stylization-to-stylization relation." 
Aside from the losses from the baseline SANET(Style loss, Content loss, Adv. Loss), the authors employ two contrastive losses.
In the style contrastive loss, two stylized from the same style image are pulled together while rest pairs are pushed. Same for content contrastive loss, two stylized images from the same content image are pulled together.

Architecture             |  Results
:-------------------------:|:-------------------------:
<img src="https://user-images.githubusercontent.com/70194535/172903218-e22cc42b-2522-4d86-b4e7-65a5f1d0aba0.png" alt="drawing" width="500"/>|<img src="https://user-images.githubusercontent.com/70194535/172910387-f251e28e-1c67-4928-9295-440a0c96a7e2.png" alt="drawing" width="400"/>

### Downsides
- Need style dataset, need training, inference can be done only with styles from the training set.

## Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning

I will just summarize the takeaways. activations-based normalizers prevent exploding variance of forward activations in ResNet.
"The ability of a normalizer to generate dissimilar activations for different inputs is a strong predictor of optimization speed."
GroupNorm can prevent high similarity of activations for different inputs when the group size is small. 
This means, instance normalization is likely to produce informative activations. However, instance normalization suffers from unstable backward propagation.
Some words on informative forward propagation. Different inputs should produce different activations.
It is surprising that correlation between mean training accuracy and the average cosine simlarity of activations is high.
Due to this property, "for any given network architecture, one can predict which normalizer will enable the fastest convergence without even training the model."
Then, a normalizer that produces good initial dissimilar representations contributes to an healthy training.
Because LayerNorm produces similar activations in a layer, it doesnt do well in CNN classifiers
(But why sequential architectures use LayerNorm?).

