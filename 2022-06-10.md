# 2022-06-10

## Artistic Style Transfer with Internal-external Learning and Contrastive Learning (NeurIPS 2021)

Further develops SANET, an encoder-decoder architecture for style transfer which takes input content and style image, and outputs a mixed image. 
Here, the paper argues that style(content) image should have closer relations to another image with the same style(content), and they call this "stylization-to-stylization relation." 
Aside from the losses from the baseline SANET(Style loss, Content loss, Adv. Loss), the authors employ two contrastive losses.
In the style contrastive loss, two stylized from the same style image are pulled together while rest pairs are pushed. Same for content contrastive loss, two stylized images from the same content image are pulled together.

<img src="https://user-images.githubusercontent.com/70194535/172903218-e22cc42b-2522-4d86-b4e7-65a5f1d0aba0.png" alt="drawing" width="500"/>|<img src="https://user-images.githubusercontent.com/70194535/172910387-f251e28e-1c67-4928-9295-440a0c96a7e2.png" alt="drawing" width="300"/>

### Downsides
- Need style dataset, need training, inference can be done only with styles from the training set.

## Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning (NeurIPS 2021)

I will just summarize the takeaways. Nomalization layers prevent exploding variance of forward activations in ResNet by 
"The ability of a normalizer to generate dissimilar activations for different inputs is a strong predictor of optimization speed."
GroupNorm can prevent high similarity of activations for different inputs when the group size is small. 
This means, instance normalization is likely to produce informative activations. However, instance normalization suffers from unstable backward propagation.
Some words on informative forward propagation. Different inputs should produce different activations.
It is surprising that correlation between mean training accuracy and the average cosine simlarity of activations is high.
Due to this property, "for any given network architecture, one can predict which normalizer will enable the fastest convergence without even training the model."
Then, a normalizer that produces good initial dissimilar representations contributes to an healthy training.
Because LayerNorm produces similar activations in a layer, it doesnt do well in CNN classifiers
(But why sequential architectures use LayerNorm?).

## Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Adaptation
- Given vector/matrix X and Y, AdaIN adapts X's mean and std with Y's. This approach arises from a basic assumption; the feature map is Gaussian, hence mean and std is sufficient to express the distribution of Y. But in reality, Y is commonly multimodal and skewened, so adapting higher moments (e.g. skewness, kurtosis) would be useful. 
- To match these, Exact Feature Distribution Matching is used(EFDM). The illustration below explains how we adapt all moments through the algorithm. That means, the algorithm output (vector o) has the mean, std, skewness, kurtosis values of vector y, while following the underlying distribution of vector x.
- Algorithms that makes use of AdaIN can be "upgraded" with EFDM.
## Encoding Robustness to Image Style via Adversarial Feature Perturbations (NeurIPS 2021)

Very simple paper. Adversarially robust networks are produced by adversarially perturbing the training data in the RGB space and using it as training data. Here, instead of perturbing in the RGB space, they perturb in the feature space(more especifically, the mean and std of the feature maps). This gives robustness against domain shift, style variation and image corruption. It increases accuracy for some hard datasets. The side effect of this is that accuracy in in-domain testset decreases.

<img src="https://user-images.githubusercontent.com/70194535/172991353-ce5778b3-7d9d-4402-afc6-b9505e64f5a8.png" alt="drawing" width="700"/>

## Inverse Problem Leveraging Pre-trained Contrastive Representations
- R(x) is CLIP's embedding of an image, A(x) is a highly corrupted image. We want to approximate R(x). Lets say we dont have x and have only A(x). Then, we want to train some encoder S, which S(A(X)) is approximate to R(x). The training procedure is simple: use a siamese network. Input to the teacher(CLIP) is not corrupted while input to the student(encoder S) is highly corrupted. 
- With encoder S and a classifier head, it can classify very corrupted inputs very well.

<img src="https://user-images.githubusercontent.com/70194535/173176495-76b3b30f-ad17-45eb-a2b6-73f4345f41e3.png" alt="drawing" width="400"/>|<img src="https://user-images.githubusercontent.com/70194535/173176669-84c42665-61d0-4b05-949e-1c4c3b626fac.png" alt="drawing" width="300"/>

## RESTRICTING THE FLOW: INFORMATION BOTTLENECKS FOR ATTRIBUTION

- In a discriminator, we want to see which pixel contributes to the prediction of the output. A way around working with image space directly is to find the features that contributes the most to the prediction. To find these features, they insert noise to all channels in some layer, threshold by a learnable parameter lambda. By optimizing the mutual information loss below, we essentially apply more noise to feature maps that contribute less to the prediction and less noise to the ones with high contribution. Then, we average over all thresholded channels to estimate input attribution.

![image](https://user-images.githubusercontent.com/70194535/173177376-5b96d9d7-9142-4b89-9325-a16fc9a9cf89.png)
![image](https://user-images.githubusercontent.com/70194535/173177711-764e8cb1-1557-4aad-bda8-e69a7f3395eb.png)
<img src="https://user-images.githubusercontent.com/70194535/173177376-5b96d9d7-9142-4b89-9325-a16fc9a9cf89.png" alt="drawing" width="600"/>
<img src="https://user-images.githubusercontent.com/70194535/173177711-764e8cb1-1557-4aad-bda8-e69a7f3395eb.png" alt="drawing" width="400"/>
