# 2022-06-10

## Artistic Style Transfer with Internal-external Learning and Contrastive Learning (NeurIPS 2021)

Further develops SANET, an encoder-decoder architecture for style transfer which takes input content and style image, and outputs a mixed image. 
Here, the paper argues that style(content) image should have closer relations to another image with the same style(content), and they call this "stylization-to-stylization relation." 
Aside from the losses from the baseline SANET(Style loss, Content loss, Adv. Loss), the authors employ two contrastive losses.
In the style contrastive loss, two stylized from the same style image are pulled together while rest pairs are pushed. Same for content contrastive loss, two stylized images from the same content image are pulled together.

<img src="https://user-images.githubusercontent.com/70194535/172903218-e22cc42b-2522-4d86-b4e7-65a5f1d0aba0.png" alt="drawing" width="500"/>|<img src="https://user-images.githubusercontent.com/70194535/172910387-f251e28e-1c67-4928-9295-440a0c96a7e2.png" alt="drawing" width="400"/>

### Downsides
- Need style dataset, need training, inference can be done only with styles from the training set.

## Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning (NeurIPS 2021)

I will just summarize the takeaways. activations-based normalizers prevent exploding variance of forward activations in ResNet.
"The ability of a normalizer to generate dissimilar activations for different inputs is a strong predictor of optimization speed."
GroupNorm can prevent high similarity of activations for different inputs when the group size is small. 
This means, instance normalization is likely to produce informative activations. However, instance normalization suffers from unstable backward propagation.
Some words on informative forward propagation. Different inputs should produce different activations.
It is surprising that correlation between mean training accuracy and the average cosine simlarity of activations is high.
Due to this property, "for any given network architecture, one can predict which normalizer will enable the fastest convergence without even training the model."
Then, a normalizer that produces good initial dissimilar representations contributes to an healthy training.
Because LayerNorm produces similar activations in a layer, it doesnt do well in CNN classifiers
(But why sequential architectures use LayerNorm?).

## Encoding Robustness to Image Style via Adversarial Feature Perturbations (NeurIPS 2021)

Very simple paper. Adversarially robust networks are produced by adversarially perturbing the training data in the RGB space and using it as training data. Here, instead of perturbing in the RGB space, they perturb in the feature space(more especifically, the mean and std of the feature maps). This gives robustness against domain shift, style variation and image corruption. It increases accuracy for some hard datasets. The side effect of this is that accuracy in in-domain testset decreases.
<img src="https://user-images.githubusercontent.com/70194535/172991353-ce5778b3-7d9d-4402-afc6-b9505e64f5a8.png" alt="drawing" width="700"/>
